论文地址 [[SAFE RLHF- SAFE REINFORCEMENT LEARNING FROM HUMAN FEEDBACK.pdf]]
创新点：
1. 解耦了安全性和有用性，将原来的单一目标转换为多目标问题。
   1. 标注数据的时候，分别标注有用偏好和无害偏好。
   2. 分别训练了一个有用性奖励模型和无害奖励模型，其中无害模型还对 0 值进行了校准。
2. 利用拉格朗日乘子法，将带约束的最优化问题转化为无约束的最优化问题，交替更新模型参数和惩罚因子，使得模型更快的收敛。

评估方面：
1. 测试数据集：
   1. 自己设计的数据集。
   2. 没有经过训练的测试数据集。
   3. 从红队中选择10%的提示。
2. LLM AS judge 可以评估对比进行 RLHF 后的模型和 RLHF 之前的模型的 ELo 分数。
3. 训练一个奖励模型，计算模型在测试数据集上的答案，让奖励模型打分，计算奖励模型输出的分布。

问题：
1. 整个过程只进行了 RLHF 的优化，不涉及 PCT、SFT 优化。为什么没有用 beavertails 中的分类数据进行 SFT 优化？
2. 三阶段的 RLHF 微调的设计动机是什么？是否是为了模拟线上情况？
3. 在训练有害奖励模型的时候，为什么要对标签的正确性进行惩罚，理论上有用性和安全性可以完全对称？是否有用也可以加惩罚？
4. 在训练模型的时候，拉格朗日乘子是否是和模型参数同时更新的？
5. RLHF 的数据应该加入多少？
6. 你们如何判断 RLHF 阶段的停止？
7. beavertails中的数据，和 SAFE RLHF中的是一样的吗？那么对于不同的人如果判断标注结果不一致，会如何处理？ 
8. 论文中有提到后续会进行多轮对话/更多目标的工作，这块的工作有进行吗？

细节：
1. 再进行 RLHF 过程中，safe 的数据加入了 30%。再后面的阶段，因为模型回答的越来越安全，因此加入了更多的红队攻击。


复刻这篇论文我需要：
1. 定义好安全问题的分类
2. 根据分类，收集 prompt 数据
	1. 需要多少 prompt 数据
3. 让模型对 prompt 数据生成答案
	1. 模型需要生成几个答案
	2. 如果生成的答案分布比较一致该怎么办？调整温度系数？比如都是不安全的答案或者都是安全的答案？
	3. 能不能让 GPT4 生成不同的答案？
4. 招募众包，对答案数据进行标注
	1. 每个众包需要标注几个数据？
	2. 标注标准如何？
	3. 如何验收？
5. 训练 RM 模型
	1. RM 模型的分类准确性？因为初始的 SFT 模型可能分布比较有限，那么 RM 模型的判断能力是否也比较有限？还是说不同阶段，训练不同的 RM 模型，迭代式的更新 RM 模型？
	2. 应该是不更新 RM 模型？
6. PPO
7. 测试

扩展到多轮对话上：
1. 问题 prompt 构造。
	1. 在原有的问题数据集上，进行日语翻译，或者拿 donot anwser 数据集。
2. 不同长度的长对话构造。
测试：
1. 单轮 prompt 的效果。
2. 多轮 prompt 的效果。

